
---
title: "ex06_MarcoLorenzetti"
author: "Marco Lorenzetti"
date: '2022-06-05'
output: html_document
---

```{r}
library("ggplot2")
library("ggpubr")
library("tidyverse")
library("rjags")

```

### Exercise 1

**a)** Only two possible results for the method (it succeed or it fails) and each trial is indipendent from the other ones, then the best probability distribution for y is the Binomial distribution:

$$
\text{P}(y|n,p) = {n\choose y} p^y(1-p)^{n-y}
$$

**b)** The frequentist estimator of the failure probability of the new method is simply calculated dividing the number of fails by the total patients:

$$
p_{f}=\frac{y}{n}=\frac{6}{75}=0.08
$$

**c)** Given the $\mu$ and $\sigma$ of the prior Beta distribution is possible to obtain its $\alpha$ and $\beta$ parameters with:

$$
\alpha=(\frac{1-\mu}{\sigma^{2}}-\frac{1}{\mu})\mu^{2}\\
\beta=\alpha(\frac{1}{\mu}-1)
$$

and then calculate the posterior parameters $\alpha'$ and $\beta'$ as:

$$
\alpha'=y+\alpha=6+\alpha\\
\beta'=n-y+\beta=75-6+\beta
$$

```{r}
y <- 6
n <- 75
mu.prior <- 0.15
sigma.prior <- 0.14

# this function returns a list with the alpha and beta parameters given mu and sigma
AlphaBeta_estimator <- function(mu, sigma) {
  alpha <- (((1 - mu) / (sigma ^ 2)) - 1 / mu) * (mu ^ 2)
  beta <- (1-mu)*((mu*(1-mu)/sigma**2)-1)
  return(c(alpha, beta))
}

pars <- AlphaBeta_estimator(mu.prior, sigma.prior)
alpha.prior <- pars[1]
beta.prior <- pars[2]
alpha.post <- y + alpha.prior
beta.post <- n - y + beta.prior

samples <- 400
seq <- seq(0, 1, by=1/samples)
# Beta Posterior distribution
Beta.posterior <- dbeta(seq, alpha.post, beta.post)

# plots
plot(seq, Beta.posterior, type='l', col='blue', xlim=c(0,0.5))

p.MLE <- (y + alpha.prior - 1)/(n + alpha.prior + beta.prior -2)
sigma.MLE <- (1/(n + alpha.prior + beta.prior -2))*(((alpha.prior + y - 1)/(alpha.prior + y))^(1/2)) #!!sigma sbagliato, troppo piccolo
abline(v = p.MLE, col = "gray60")
abline(v = p.MLE+sigma.MLE, col = "red",  lty = 'dashed')
abline(v = p.MLE-sigma.MLE, col = "red",  lty = 'dashed')
```

**d)** In order to implement an hypothesis testing we need to formulate the *null* and *alternative hypothesis* : $$
H_0: \qquad p < 0.15\\
H_1: \qquad p \geq 0.15
$$ Our goal is understand if we should reject or not the *null hypothesis* due to the evidence of data. To do it in the Bayesian way we have to compute the probability of $H_0$ integrating the posterior multiplied by $p$ from 0 to the probability given for the old test ($\mu_{old}=0.15$):

```{r}
mu.old <- 0.15
sign_level <- 0.05
prob_H0 <- integrate(function(x) {x*dbeta(x, alpha.post, beta.post)}, 0, mu.old)
cat('The probability of H0 is:', prob_H0$value*100, '% \nThe significance level is:', sign_level*100, '%\n')

if (prob_H0$value>sign_level) { print('Null Hypothesis can\'t be rejected by the evidence') } else { 
    print('Null Hypothesis is rejected by the evidence') }
```

**e)** One method to impplement the frequentist way is compute the *Z score* as

$$
Z=\frac{p_f- \mu_{old}}{\sigma/\sqrt{n}}
$$

where $\sigma$ is obtained computing the square root of the binomial variance: $\sqrt{np(1-p)}$. If $Z$ is equal or lower than the *z-score* relative to $95\%$ of confidence level ($-1.96$) the null hypothesis is rejected

```{r}
z.score.cl <- -1.96
pf <- y/n
sigma.test <- (n*pf*(1-pf))^(1/2)
z <- (pf-mu.old)/(sigma.test/(n^(1/2)))
cat('z: ', z, '\nz-score for 95% confidence level: ', z.score.cl, '\n')
if (z.score.cl>z) { print('Null Hypothesis can\'t be rejected by the evidence') } else { 
    print('Null Hypothesis is rejected by the evidence') }
```

### Exercise 2

**a)**

```{r}
death_soldiers <- c(0,1,2,3,4,5)
n1_obs <- c(109, 65, 22, 3, 1, 0)
n2_obs <- c(144, 91, 32, 11, 2, 0)
alpha1.unif <- sum(death_soldiers*n1_obs) + 1
alpha2.unif <- sum(death_soldiers*n2_obs) + 1
#gamma functions
n_samples <- 300
lambda_seq <- seq(0, 1, length.out = n_samples)
Gamma1.Post.Unif_Prior <- dgamma(lambda_seq, alpha1.unif, sum(n1_obs))
Gamma2.Post.Unif_Prior <- dgamma(lambda_seq, alpha2.unif, sum(n2_obs))
#means, medians and variances
mean1 <- alpha1.unif/sum(n1_obs)
sigma1 <- (alpha1.unif/(sum(n1_obs)^2))^(1/2)
median1 <- qgamma(0.5, alpha1.unif, sum(n1_obs))
CI1.min <- qgamma(0.025, alpha1.unif, sum(n1_obs))
CI1.max <- qgamma(0.975, alpha1.unif, sum(n1_obs))
mean2 <- alpha2.unif/sum(n2_obs)
sigma2 <- (alpha2.unif/(sum(n2_obs)^2))^(1/2)
median2 <- qgamma(0.5, alpha2.unif, sum(n2_obs))
CI2.min <- qgamma(0.025, alpha2.unif, sum(n2_obs))
CI2.max <- qgamma(0.975, alpha2.unif, sum(n2_obs))

cat('1st observations:  mean=', mean1, ', variance=', sigma1^2,', median=', median1, ', C.I. =', CI1.min, '-', CI1.max)
cat('\n2nd observations:  mean=', mean2, ', variance=', sigma2^2,', median=', median2, ', C.I. =', CI2.min, '-', CI2.max)
```

```{r}
# gamma 1
plot(lambda_seq, Gamma1.Post.Unif_Prior, type='l', col='blue')
abline(v = mean1, col = "slateblue3")
abline(v = mean1+sigma1, col = "slateblue3",  lty = 'dashed')
abline(v = mean1-sigma1, col = "slateblue3",  lty = 'dashed')
abline(v = median1, col = "seagreen4",  lty = 'dashed')
# abline(v = CI1.min, col = "slateblue2",  lty = 'solid')
# abline(v = CI1.max, col = "slateblue2",  lty = 'solid')
polygon(c(lambda_seq[lambda_seq>=CI1.max], max(lambda_seq), CI1.max), 
        c(Gamma1.Post.Unif_Prior[lambda_seq>=CI1.max], 0, 0), col="slateblue2")
polygon(c(lambda_seq[lambda_seq<=CI1.min], CI1.min, 0), 
        c(Gamma1.Post.Unif_Prior[lambda_seq<=CI1.min],0, 0),
        col="slateblue2")
# gamma 2
plot(lambda_seq, Gamma2.Post.Unif_Prior, type='l', col='red')
abline(v = mean2, col = "orangered3")
abline(v = mean2+sigma2, col = "orangered3",  lty = 'dashed')
abline(v = mean2-sigma2, col = "orangered3",  lty = 'dashed')
abline(v = median2, col = "orangered1",  lty = 'dashed')
polygon(c(lambda_seq[lambda_seq>=CI2.max], max(lambda_seq), CI2.max), 
        c(Gamma2.Post.Unif_Prior[lambda_seq>=CI2.max], 0, 0), col="red")
polygon(c(lambda_seq[lambda_seq<=CI2.min], CI2.min, 0), 
        c(Gamma2.Post.Unif_Prior[lambda_seq<=CI2.min],0, 0),
        col="red")
```

**b)** Using a Jeffrey's Prior

```{r}
alpha1.jeff <- sum(death_soldiers*n1_obs) + 1/2
alpha2.jeff <- sum(death_soldiers*n2_obs) + 1/2

Gamma1.Post.Jeff_Prior <- dgamma(lambda_seq, alpha1.unif, sum(n1_obs))
Gamma2.Post.Jeff_Prior <- dgamma(lambda_seq, alpha2.unif, sum(n2_obs))
#means, medians and variances
mean1.j <- alpha1.jeff/sum(n1_obs)
sigma1.j <- (alpha1.jeff/(sum(n1_obs)^2))^(1/2)
median1.j <- qgamma(0.5, alpha1.jeff, sum(n1_obs))
CI1.min.j <- qgamma(0.025, alpha1.jeff, sum(n1_obs))
CI1.max.j <- qgamma(0.975, alpha1.jeff, sum(n1_obs))
mean2.j <- alpha2.jeff/sum(n2_obs)
sigma2.j <- (alpha2.jeff/(sum(n2_obs)^2))^(1/2)
median2.j <- qgamma(0.5, alpha2.jeff, sum(n2_obs))
CI2.min.j <- qgamma(0.025, alpha2.jeff, sum(n2_obs))
CI2.max.j <- qgamma(0.975, alpha2.jeff, sum(n2_obs))

cat('1st observations:  mean=', mean1.j, ', variance=', sigma1.j^2,', median=', median1.j, ', C.I. =', CI1.min.j, '-', CI1.max.j)
cat('\n2nd observations:  mean=', mean2.j, ', variance=', sigma2.j^2,', median=', median2.j, ', C.I. =', CI2.min.j, '-', CI2.max.j)
```

```{r}
# gamma 1
plot(lambda_seq, Gamma1.Post.Jeff_Prior, type='l', col='blue')
abline(v = mean1.j, col = "slateblue3")
abline(v = mean1.j+sigma1.j, col = "slateblue3",  lty = 'dashed')
abline(v = mean1.j-sigma1.j, col = "slateblue3",  lty = 'dashed')
abline(v = median1.j, col = "seagreen4",  lty = 'dashed')
# abline(v = CI1.min, col = "slateblue2",  lty = 'solid')
# abline(v = CI1.max, col = "slateblue2",  lty = 'solid')
polygon(c(lambda_seq[lambda_seq>=CI1.max.j], max(lambda_seq), CI1.max.j), 
        c(Gamma1.Post.Jeff_Prior[lambda_seq>=CI1.max.j], 0, 0), col="slateblue2")
polygon(c(lambda_seq[lambda_seq<=CI1.min.j], CI1.min.j, 0), 
        c(Gamma1.Post.Jeff_Prior[lambda_seq<=CI1.min.j],0, 0),
        col="slateblue2")
# gamma 2
plot(lambda_seq, Gamma2.Post.Jeff_Prior, type='l', col='red')
abline(v = mean2.j, col = "orangered3")
abline(v = mean2.j+sigma2.j, col = "orangered3",  lty = 'dashed')
abline(v = mean2.j-sigma2.j, col = "orangered3",  lty = 'dashed')
abline(v = median2.j, col = "orangered1",  lty = 'dashed')
polygon(c(lambda_seq[lambda_seq>=CI2.max.j], max(lambda_seq), CI2.max.j), 
        c(Gamma2.Post.Jeff_Prior[lambda_seq>=CI2.max.j], 0, 0), col="red")
polygon(c(lambda_seq[lambda_seq<=CI2.min.j], CI2.min.j, 0), 
        c(Gamma2.Post.Jeff_Prior[lambda_seq<=CI2.min.j],0, 0),
        col="red")
```

### Exercise 3

**a)** As seen in exercise 1 the frequentit probability can be simply computed diving y by n: $p_f=\frac{11}{116}=0.0948$

**b) - c)** Using a Beta-Prior on a Bernoulli Process we obtain a Beta Posterior:

```{r}
y <- 11
n <- 116
pf <- y/n
a <- 1
b <- 10
n_samples = 300
p_seq <- seq(0, 1, by=0.001)

mean <- (a+y)/(a+b+n)
a_ <- a+y
b_ <- n-y+b
s <- (a_*b_)/((a_+b_)*(a_+b_+1))
median <- qbeta(0.5, a_, b_)
CI.min <- qbeta(0.025, a_, b_)
CI.max <- qbeta(0.975, a_, b_)
cat('alpha posterior: ', a_, '  -  beta posterior: ', b_)
cat('\nmean=', mean, ', variance=', s^2,', median=', 
    median, ', C.I. =', CI.min, '-', CI.max)


Posterior <- dbeta(p_seq, a+y, n-y+b)
plot(p_seq, Posterior, type='l', col='blue')
abline(v = mean, col = "orangered3")
abline(v = CI.max, col = "orangered3",  lty = 'dashed')
# abline(v = mean-s, col = "orangered3",  lty = 'dashed')
abline(v = median, col = "orangered1",  lty = 'dashed')
polygon(c(p_seq[p_seq>=CI.max], max(p_seq), CI.max), 
        c(Posterior[p_seq>=CI.max], 0, 0), col="blue")
polygon(c(p_seq[p_seq<=CI.min], CI.min, 0), 
        c(Posterior[p_seq<=CI.min],0, 0),
        col="blue")
```

The Bayesian estimator can be found as:

$$
p_0=\frac{1}{\alpha + \beta + n - 2}\cdot\sqrt{\frac{\alpha + y -1}{\alpha + y}}
$$

```{r}
p0.bayesian <- (1/(a+b+n-2))*(((a + y - 1)*(a + y))^(1/2))
p0.bayesian
```

**d)** Frequentist approach:

```{r}
l <- qbinom(0.025, n, 0.1)
r <- qbinom(0.975, n, 0.1)

left <- function(l){pbinom(l, n, 0.1)}
right <- function(r){1- pbinom(r, n, 0.1)}

cat('\n Real values are (',l,',', r,') that lead to an area of', left(l) + right(r))

#different combination of left and right
for (i in seq(-1, 1, 1)){
   cat('\n The cuts (',l-i,',', r,') give an area of', left(l-i) + right(r))
   cat('\n The cuts (',l,',', r-i,') give an area of', left(l) + right(r-i))
   cat('\n The cuts (',l-i,',', r-i,') give an area of', left(l-i) + right(r-i))
   cat('\n The cuts (',l+i,',', r-i,') give an area of', left(l+i) + right(r-i))
   cat('\n The cuts (',l+i,',', r+i,') give an area of', left(l+i) + right(r+i))
    
}
```

The best choice is to select as rejection region $[5,18]$ that has the most close value to $0.05$. We can see that $y = 11$ lies in the acceptance rejon and then we cannot reject the null hypothesis

Bayesian Approach: $p_0 = 0.10$ lies inside the $95 \%$ credible interval for our Posterior distribution $[0.05018522 - 0.1508379]$ then we cannot reject the null hypothesis.

**e)** using *Beta(1,10)* prior

```{r}
y <- 9
n <- 165
pf <- y/n
a <- 1
b <- 10
n_samples = 300
p_seq <- seq(0, 1, by=0.001)

mean <- (a+y)/(a+b+n)
a_ <- a+y
b_ <- n-y+b
s <- (a_*b_)/((a_+b_)*(a_+b_+1))
median <- qbeta(0.5, a_, b_)
CI.min <- qbeta(0.025, a_, b_)
CI.max <- qbeta(0.975, a_, b_)
cat('mean=', mean, ', variance=', s^2,', median=', 
    median, ', C.I. =', CI.min, '-', CI.max)


Posterior2.beta_pr <- dbeta(p_seq, a+y, n-y+b)
plot(p_seq, Posterior2.beta_pr, type='l', col='blue')
abline(v = mean, col = "orangered3")
abline(v = median, col = "orangered1",  lty = 'dashed')
polygon(c(p_seq[p_seq>=CI.max], max(p_seq), CI.max), 
        c(Posterior2.beta_pr[p_seq>=CI.max], 0, 0), col="blue")
polygon(c(p_seq[p_seq<=CI.min], CI.min, 0), 
        c(Posterior2.beta_pr[p_seq<=CI.min],0, 0),
        col="blue")
```

```{r}
#bayesian estimator
p0.bayesian.betapr <- (1/(a+b+n-2))*(((a + y - 1)*(a + y))^(1/2))
p0.bayesian.betapr
```

Using the posterior found with older measuraments: $\alpha_{post}=12$, $\beta_{post}=115$

```{r}
y <- 9
n <- 165
pf <- y/n
a_post <- 12
b_post <- 115
n_samples = 300
p_seq <- seq(0, 1, by=0.001)

mean <- (a_post+y)/(a_post+b_post+n)
a_ <- a_post+y
b_ <- n-y+b_post
s <- (a_*b_)/((a_+b_)*(a_+b_+1))
median <- qbeta(0.5, a_, b_)
CI.min <- qbeta(0.025, a_, b_)
CI.max <- qbeta(0.975, a_, b_)
cat('mean=', mean, ', variance=', s^2,', median=', 
    median, ', C.I. =', CI.min, '-', CI.max)
```

```{r}
Posterior2.post_pr <- dbeta(p_seq, a_post+y, n-y+b_post)
plot(p_seq, Posterior2.post_pr, type='l', col='blue')
abline(v = mean, col = "orangered3")
abline(v = median, col = "orangered1",  lty = 'dashed')
polygon(c(p_seq[p_seq>=CI.max], max(p_seq), CI.max), 
        c(Posterior2.post_pr[p_seq>=CI.max], 0, 0), col="blue")
polygon(c(p_seq[p_seq<=CI.min], CI.min, 0), 
        c(Posterior2.post_pr[p_seq<=CI.min],0, 0),
        col="blue")
```

```{r}
#bayesian estimator
p0.bayesian.postpr <- (1/(a_post+b_post+n-2))*(((a_post + y - 1)*(a_post + y))^(1/2))
p0.bayesian.postpr
```

### Exercise 4

```{r}
library("rjags")
```

```{r}
n <- 75 #number of patients
y <- 6 # false negative


#mu.prior sigma.prior already written
#copying ex1 values
alpha.prior <- pars[1]
beta.prior <- pars[2]
alpha.post <- y + alpha.prior
beta.post <- n - y + beta.prior

#mean and std: p.MLE, sigma.MLE

#define data vector
data_j <- NULL
data_j$a_prior<- alpha.prior
data_j$b_prior <- beta.prior
data_j$X <- y
data_j$n <- n


# model creation
jagsmodel <- jags.model("mod4.bug", data_j, n.adapt=1000, quiet=TRUE)
mc_chain<- coda.samples(jagsmodel, c("p"), n.iter=10000, progress.bar="none")
print(summary(mc_chain))

mean.post <- unname(summary(mc_chain)$statistics["Mean"])
sd.post <- unname(summary(mc_chain)$statistics["SD"])


CI.an <- c(qbeta(0.025, alpha.post, beta.post), qbeta(0.975, alpha.post, beta.post))
CI.mc <- unname(summary(mc_chain)$quantiles[c("2.5%","97.5%")])


cat('Analytical analysis \n')
cat('Mean:', p.MLE)
cat('   Var: ', sigma.MLE**2)
cat('\n 95% confidence interval: [', round(CI.an[1],3),',', round(CI.an[2],3), ']')


cat('\n\n\n Montecarlo analysis \n')
cat('Mean:', mean.post)
cat('   Var: ', sd.post**2)
cat('\n 95% confidence interval: [', round(CI.mc[1],3),',', round(CI.mc[2],3), ']')

```

```{r}
#------------------PLOTS-----------------#

library("coda")
plot(mc_chain)

```

### Exercise 5

```{r}

data_1 <- rep(x = death_soldiers, times = n1_obs)
data_2 <- rep(x = death_soldiers, times = n2_obs)


data_N1 <- NULL
data_N1$X <- data_1
data_N2 <- NULL
data_N2$X <- data_2
jagsmodelN1 <- jags.model("mod5_j.bug", data_N1, quiet = TRUE)
update(jagsmodelN1, 1000, progress.bar="none")
mc_chainN1<- coda.samples(jagsmodelN1, c("lambda"), n.iter=10000, progress.bar="none")
jagsmodelN2 <- jags.model("mod5_u.bug", data_N2, quiet = TRUE)
update(jagsmodelN2, 1000, progress.bar="none")
mc_chainN2 <- coda.samples(jagsmodelN2, c("lambda"), n.iter=10000, progress.bar="none")


cat('\nSummary of chain of the 1st observations:\n ')

print(summary(mc_chainN1))



mean.post_N1 <- unname(summary(mc_chainN1)$statistics["Mean"])
sd.post_N1 <- unname(summary(mc_chainN1)$statistics["SD"])
CI.mc_N1 <- unname(summary(mc_chainN1)$quantiles[c("2.5%","97.5%")])

cat('\n\n\n Montecarlo analysis for the first observer\n')
cat('Mean:', mean.post_N1)
cat('   Var: ', sd.post_N1**2)
cat('\n 95% confidence interval: [', round(CI.mc_N1[1],3),',', round(CI.mc_N1[2],3), ']')
plot(mc_chainN1)
cat('\n--------------------------------------------\n\n')

cat('\n\n Summary of chain of the 2nd observations:\n ')

print(summary(mc_chainN2))

mean.post_N2 <- unname(summary(mc_chainN2)$statistics["Mean"])
sd.post_N2 <- unname(summary(mc_chainN2)$statistics["SD"])
CI.mc_N2 <- unname(summary(mc_chainN2)$quantiles[c("2.5%","97.5%")])

cat('\n\n\n Montecarlo analysis for the second observer\n')
cat('Mean:', mean.post_N2)
cat('   Var: ', sd.post_N2**2)
cat('\n 95% confidence interval: [', round(CI.mc_N2[1],3),',', round(CI.mc_N2[2],3), ']')
plot(mc_chainN2)

```


### Exercise 6

```{r}
data.3 <- NULL
data.3$alpha.prior<- 1
data.3$beta.prior <- 10
data.3$y <- 11
data.3$n <- 116


# model creation
jagsmodel3 <- jags.model("mod6.bug", data.3, n.adapt=1000, quiet=TRUE)
mc_chain3<- coda.samples(jagsmodel3, c("p"), n.iter=10000, progress.bar="none")
print(summary(mc_chain3))

mean.post3 <- unname(summary(mc_chain3)$statistics["Mean"])
sd.post3 <- unname(summary(mc_chain3)$statistics["SD"])
CI.mc3 <- unname(summary(mc_chain3)$quantiles[c("2.5%","97.5%")])

cat('\n\n\n Montecarlo analysis \n')
cat('Mean:', mean.post3)
cat('   Var: ', sd.post3**2)
cat('\n 95% confidence interval: [', round(CI.mc3[1],3),',', round(CI.mc3[2],3), ']')

```

```{r}
plot(mc_chain3)
```
